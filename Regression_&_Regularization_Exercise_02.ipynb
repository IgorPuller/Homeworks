{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chEqqJbLzFew"
      },
      "source": [
        "# Ydata Data Science School\n",
        "## Linear Regression & Regularization Exercise.\n",
        "\n",
        "\n",
        "## Outline\n",
        "In this exercise you will learn the following topics:\n",
        "\n",
        "1. Refresher on how linear regression is solved in batch and in Gradient Descent\n",
        "2. Implementation of Ridge Regression\n",
        "3. Comparing Ridge, Lasso and vanila Linear Regression on a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9UFmk2greT"
      },
      "source": [
        "## Refresher on Ordinary Least Square (OLS) aka Linear Regeression\n",
        "\n",
        "### Lecture Note\n",
        "\n",
        "In Matrix notation, the matrix $X$ is of dimensions $n \\times p$ where each row is an example and each column is a feature dimension.\n",
        "\n",
        "Similarily, $y$ is of dimension $n \\times 1$ and $w$ is of dimensions $p \\times 1$.\n",
        "\n",
        "The model is $\\hat{y}=X\\cdot w$ where we assume for simplicity that $X$'s first columns equals to 1 (one padding), to account for the bias term.\n",
        "\n",
        "Our objective is to optimize the loss $L$ defines as resiudal sum of squares (RSS):\n",
        "\n",
        "$L_{RSS}=\\frac{1}{N}\\left\\Vert Xw-y \\right\\Vert^2$ (notice that in matrix notation this means summing over all examples, so $L$ is scalar.)\n",
        "\n",
        "To find the optimal $w$ one needs to derive the loss with respect to $w$.\n",
        "\n",
        "$\\frac{\\partial{L_{RSS}}}{\\partial{w}}=\\frac{2}{N}X^T(Xw-y)$ (to see why, read about [matrix derivatives](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf) or see class notes )\n",
        "\n",
        "Thus, the gardient descent solution is $w'=w-\\alpha \\frac{2}{N}X^T(Xw-y)$.\n",
        "\n",
        "Solving $\\frac{\\partial{L_{RSS}}}{\\partial{w}}=0$ for $w$ one can also get analytical solution:\n",
        "\n",
        "$w_{OLS}=(X^TX)^{-1}X^Ty$\n",
        "\n",
        "The first term, $(X^TX)^{-1}X^T$ is also called the pseudo inverse of $X$.\n",
        "\n",
        "See [lecture note from Stanford](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA3MEKz80vdy"
      },
      "source": [
        "## Exercise 1 - Ordinary Least Square\n",
        "* Get the boston housing dataset https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n",
        "\n",
        "* What is $p$? what is $n$ in the above notation? hint: [shape](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ndarray.shape.html)\n",
        "\n",
        "* write a model `OrdinaryLinearRegression` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score` (which returns the MSE on a given sample set). Hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.\n",
        "\n",
        "* Fit the model. What is the training MSE?\n",
        "\n",
        "* Plot a scatter plot where on x-axis plot $Y$ and in the y-axis $\\hat{Y}_{OLS}$\n",
        "\n",
        "* Split the data to 75% train and 25% test 20 times. What is the average MSE now for train and test? Hint: use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) or [ShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html).\n",
        "\n",
        "* Use a t-test to proove that the MSE for training is significantly smaller than for testing. What is the p-value? Hint: use [scipy.stats.ttest_rel](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html).\n",
        "\n",
        "* Write a new class `OrdinaryLinearRegressionGradientDescent` which inherits from `OrdinaryLinearRegression` and solves the problem using gradinet descent. The class should get as a parameter the learning rate and number of iteration. Plot the class convergance. What is the effect of learning rate? How would you find number of iteration automatically? Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your features first.\n",
        "\n",
        "* The following parameters are optional (not mandatory to use):\n",
        "    * early_stop - True / False boolean to indicate to stop running when loss stops decaying and False to continue.\n",
        "    * verbose- True/False boolean to turn on / off logging, e.g. print details like iteration number and loss (https://en.wikipedia.org/wiki/Verbose_mode)\n",
        "    * track_loss - True / False boolean when to save loss results to present later in learning curve graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "ZuSS8LhcfZdn"
      },
      "outputs": [],
      "source": [
        "# * write a model `Ols` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score`.? hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.\n",
        "import numpy as np\n",
        "class Ols(object):\n",
        "  def __init__(self):\n",
        "    self.w = None\n",
        "\n",
        "  @staticmethod\n",
        "  def pad(X):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    #remeber pad with 1 before fitting\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.w = (np.linalg.pinv(X.T@X))@X.T@Y\n",
        "\n",
        "\n",
        "  def _fit(self, X, Y):\n",
        "    # optional to use this\n",
        "    pass\n",
        "\n",
        "  def predict(self, X):\n",
        "    #return wx\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    w = self.w\n",
        "    w = np.expand_dims(w,axis=-1)\n",
        "    return X@w\n",
        "\n",
        "  def _predict(self, X):\n",
        "    # optional to use this\n",
        "    pass\n",
        "\n",
        "  def score(self, X, Y):\n",
        "    #return MSE\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    n = Y.shape[0]\n",
        "    w = self.w\n",
        "    #w = np.expand_dims(w,axis=-1)\n",
        "    return 1/(n)*np.sum(np.power(X @ w - Y, 2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "np.random.seed(42)\n",
        "data = pd.read_csv('/content/Boston-house-price-data.csv')\n",
        "data.head()\n",
        "X = data.iloc[:,:-1]\n",
        "Y = data.iloc[:,-1]\n",
        "#X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)"
      ],
      "metadata": {
        "id": "hx3ar6H7N0uW"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "model = Ols();\n",
        "model.fit(X_train,Y_train);\n",
        "model.predict(X_test);\n",
        "print(model.score(X_train,Y_train))\n",
        "model.score(X_test,Y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvPmt-zjPSgq",
        "outputId": "addc6602-ff9b-44ab-d277-57c0ca326afc"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.545481487421423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.517444231209087"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(Y_test, model.predict(X_test));\n",
        "plt.xlabel('$Y$')\n",
        "plt.ylabel('$\\hat{Y}_{OLS}$');\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "O4eS4JUVdeEn",
        "outputId": "1c7937bb-4c4a-4bab-ed6c-665c408052c1"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_train = []\n",
        "mse_test = []\n",
        "for i in range(20):\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
        "  model = Ols();\n",
        "  model.fit(X_train,Y_train);\n",
        "  model.predict(X_test);\n",
        "  #w=model.w\n",
        "  #w = np.expand_dims(w,axis=-1)\n",
        "  #print(np.sum((X_test@w-Y_test)**2)/Y_test.size)\n",
        "  #print(np.sum((X_train@w-Y_test)**2)/Y_train.size)\n",
        "  mse_train.append(model.score(X_train,Y_train))\n",
        "  mse_test.append(model.score(X_test,Y_test))\n",
        "#w=model.w\n",
        "#w = np.expand_dims(w,axis=-1)\n",
        "#print(w)\n",
        "#print(np.sum((X_test@w-Y_test)**2)/Y_test.size)\n",
        "#print(np.sum((X_train@w-Y_test)**2)/Y_train.size)\n",
        "mse_test,mse_train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP2LKhKkeXPR",
        "outputId": "29152e78-e49b-47ad-ae8f-e43d4342cf2d"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([22.098694827136875,\n",
              "  17.537101208606998,\n",
              "  32.80613615026141,\n",
              "  18.307919215302153,\n",
              "  20.27597661703171,\n",
              "  28.632166568930533,\n",
              "  16.324580105219127,\n",
              "  28.052740153702025,\n",
              "  35.04182221594469,\n",
              "  29.265989933526214,\n",
              "  18.46827485505328,\n",
              "  24.96668596200068,\n",
              "  22.95527307628707,\n",
              "  31.91913652348652,\n",
              "  18.671428254993018,\n",
              "  30.514441366781313,\n",
              "  28.187562483281372,\n",
              "  38.67639356380551,\n",
              "  23.248805932193356,\n",
              "  24.19951726400097],\n",
              " [22.340057992152875,\n",
              "  23.644902820078894,\n",
              "  19.293734943653444,\n",
              "  23.502677147749118,\n",
              "  22.58560075021273,\n",
              "  19.940736032202935,\n",
              "  24.275955028826626,\n",
              "  20.337296807650137,\n",
              "  18.3310811663413,\n",
              "  19.977001817659175,\n",
              "  23.317216905276382,\n",
              "  21.35944845148022,\n",
              "  21.846648061600998,\n",
              "  19.264924147412756,\n",
              "  23.331396588943193,\n",
              "  19.572857262191196,\n",
              "  20.282895206656892,\n",
              "  17.325959098287267,\n",
              "  21.721290337937123,\n",
              "  21.62698970901816])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "stats.ttest_rel(mse_train, mse_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyGF5ldIgWzL",
        "outputId": "c0ca4180-f687-46d5-9e8a-a821a4aaba0d"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TtestResult(statistic=-2.3227663224081145, pvalue=0.03144792965171587, df=19)"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "B689kXfa1eit"
      },
      "outputs": [],
      "source": [
        "# Write a new class OlsGd which solves the problem using gradinet descent.\n",
        "# The class should get as a parameter the learning rate and number of iteration.\n",
        "# Plot the loss convergance. for each alpha, learning rate plot the MSE with respect to number of iterations.\n",
        "# What is the effect of learning rate?\n",
        "# How would you find number of iteration automatically?\n",
        "# Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your feature first.\n",
        "class Normalizer():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X):\n",
        "    #self.X_min = X.T.min(axis=1)\n",
        "    #self.X_max = X.T.max(axis=1)\n",
        "    self.X_mean = X.T.mean(axis=1)\n",
        "    self.X_std = X.T.std(axis=1)\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    #apply normalization\n",
        "    #return (X-self.X_min)/(self.X_max - self.X_min)\n",
        "    return (X-self.X_mean)/self.X_std\n",
        "\n",
        "class OlsGd(Ols):\n",
        "\n",
        "  def __init__(self, learning_rate=.05,\n",
        "               num_iteration=1000,\n",
        "               normalize=True,\n",
        "               early_stop=True,\n",
        "               verbose=True):\n",
        "\n",
        "    super(OlsGd, self).__init__()\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iteration = num_iteration\n",
        "    self.early_stop = early_stop\n",
        "    self.normalize = normalize\n",
        "    self.normalizer = Normalizer()\n",
        "    self.verbose = verbose\n",
        "\n",
        "\n",
        "  def _fit(self, X, Y, reset=True, track_loss=True):\n",
        "    #remeber to normalize the data before starting\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    norm = Normalizer()\n",
        "    norm.fit(X)\n",
        "    X = norm.predict(X)\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    n = Y.shape[0]\n",
        "    w = np.random.uniform(-1, 1, (X.shape[1]))\n",
        "    for i in range(self.num_iteration):\n",
        "      grad = 2/n * X.T @ (X @ w - Y)\n",
        "      w -= self.learning_rate*grad\n",
        "    self.w = w\n",
        "\n",
        "\n",
        "  def _predict(self, X):\n",
        "    #remeber to normalize the data before starting\n",
        "    norm = Normalizer()\n",
        "    norm.fit(self.X)\n",
        "    X = norm.predict(X)\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    w = self.w\n",
        "    w = np.expand_dims(w,axis=-1)\n",
        "    return X@w\n",
        "\n",
        "  def _step(self, X, Y):\n",
        "    # use w update for gradient descent\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(\"----------------------------\")\n",
        "  mse_train = []\n",
        "  mse_test = []\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
        "  model = OlsGd(learning_rate=.05, num_iteration=1000);\n",
        "  model._fit(X_train,Y_train);\n",
        "  model._predict(X_test);\n",
        "  mse_train.append(model.score(X_train,Y_train))\n",
        "  mse_test.append(model.score(X_test,Y_test))\n",
        "  print(model.w)\n",
        "  print(mse_test)\n",
        "  print(mse_train)\n",
        "  model = Ols();\n",
        "  model.fit(X_train,Y_train);\n",
        "  model.predict(X_test);\n",
        "  print(model.w)\n",
        "\n",
        "\n",
        "#mse_test,mse_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkHwrVH_8Up2",
        "outputId": "bcad341f-5e6c-447c-9b10-7b9ec07c1e7d"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE      DIS  RAD    TAX  \\\n",
            "480  5.82401   0.0  18.10     0  0.532  6.242  64.7   3.4242   24  666.0   \n",
            "351  0.07950  60.0   1.69     0  0.411  6.579  35.9  10.7103    4  411.0   \n",
            "248  0.16439  22.0   5.86     0  0.431  6.433  49.1   7.8265    7  330.0   \n",
            "216  0.04560   0.0  13.89     1  0.550  5.888  56.0   3.1121    5  276.0   \n",
            "429  9.33889   0.0  18.10     0  0.679  6.380  95.6   1.9682   24  666.0   \n",
            "..       ...   ...    ...   ...    ...    ...   ...      ...  ...    ...   \n",
            "30   1.13081   0.0   8.14     0  0.538  5.713  94.1   4.2330    4  307.0   \n",
            "288  0.04590  52.5   5.32     0  0.405  6.315  45.6   7.3172    6  293.0   \n",
            "80   0.04113  25.0   4.86     0  0.426  6.727  33.5   5.4007    4  281.0   \n",
            "218  0.11069   0.0  13.89     1  0.550  5.951  93.8   2.8893    5  276.0   \n",
            "20   1.25179   0.0   8.14     0  0.538  5.570  98.1   3.7979    4  307.0   \n",
            "\n",
            "     PTRATIO       B  LSTAT  \n",
            "480     20.2  396.90  10.74  \n",
            "351     18.3  370.78   5.49  \n",
            "248     19.1  374.71   9.52  \n",
            "216     16.4  392.80  13.51  \n",
            "429     20.2   60.72  24.08  \n",
            "..       ...     ...    ...  \n",
            "30      21.0  360.17  22.60  \n",
            "288     16.6  396.90   7.60  \n",
            "80      19.0  396.90   5.29  \n",
            "218     16.4  396.90  17.92  \n",
            "20      21.0  376.57  21.02  \n",
            "\n",
            "[379 rows x 13 columns]\n",
            "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
            "480  0.269444 -0.484238  1.012424 -0.287469 -0.203885 -0.037345 -0.157579   \n",
            "351 -0.422225  2.195629 -1.433157 -0.287469 -1.253745  0.448530 -1.196918   \n",
            "248 -0.412004  0.498380 -0.811702 -0.287469 -1.080214  0.238032 -0.720554   \n",
            "216 -0.426307 -0.484238  0.385008  3.469455 -0.047707 -0.547730 -0.471546   \n",
            "429  0.692655 -0.484238  1.012424 -0.287469  1.071565  0.161618  0.957546   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "30  -0.295642 -0.484238 -0.471914 -0.287469 -0.151826 -0.800039  0.903414   \n",
            "288 -0.426271  1.860645 -0.892178 -0.287469 -1.305804  0.067904 -0.846863   \n",
            "80  -0.426845  0.632373 -0.960732 -0.287469 -1.123597  0.661911 -1.283530   \n",
            "218 -0.418470 -0.484238  0.385008  3.469455 -0.047707 -0.456899  0.892587   \n",
            "20  -0.281075 -0.484238 -0.471914 -0.287469 -0.151826 -1.006211  1.047766   \n",
            "\n",
            "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
            "480 -0.179979  1.637051  1.520075  0.792646  0.444299 -0.284035  \n",
            "351  3.242045 -0.642730  0.004782 -0.065289  0.154748 -1.037962  \n",
            "248  1.887626 -0.300763 -0.476546  0.295946  0.198313 -0.459233  \n",
            "216 -0.326562 -0.528741 -0.797432 -0.923224  0.398849  0.113751  \n",
            "429 -0.863811  1.637051  1.520075  0.792646 -3.282401  1.631657  \n",
            "..        ...       ...       ...       ...       ...       ...  \n",
            "30   0.199886 -0.642730 -0.613220  1.153882  0.037131  1.419122  \n",
            "288  1.648425 -0.414752 -0.696412 -0.832915  0.444299 -0.734955  \n",
            "80   0.748313 -0.642730 -0.767720  0.250792  0.444299 -1.066683  \n",
            "218 -0.431203 -0.528741 -0.797432 -0.923224  0.444299  0.747050  \n",
            "20  -0.004466 -0.642730 -0.613220  1.153882  0.218932  1.192226  \n",
            "\n",
            "[379 rows x 13 columns]\n",
            "[22.32559367 -0.99047788  1.04281415  0.07682287  0.83346425 -1.70412058\n",
            "  2.11893376  0.36876608 -2.92204996  2.7885     -2.10990263 -2.09297919\n",
            "  0.95966424 -4.29725022]\n",
            "[452460.0762152777]\n",
            "[465948.22081549285]\n",
            "[ 3.88325794e+01 -1.19362776e-01  4.66625140e-02  1.22863643e-02\n",
            "  3.12828022e+00 -1.47920079e+01  3.05400282e+00  1.33532882e-02\n",
            " -1.37204643e+00  3.19368049e-01 -1.26313603e-02 -9.45100451e-01\n",
            "  1.06411765e-02 -6.17130734e-01]\n",
            "----------------------------\n",
            "         CRIM   ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
            "22    1.23247  0.0   8.14     0  0.538  6.142   91.7  3.9769    4  307.0   \n",
            "322   0.35114  0.0   7.38     0  0.493  6.041   49.9  4.7211    5  287.0   \n",
            "417  25.94060  0.0  18.10     0  0.679  5.304   89.1  1.6475   24  666.0   \n",
            "48    0.25387  0.0   6.91     0  0.448  5.399   95.3  5.8700    3  233.0   \n",
            "16    1.05393  0.0   8.14     0  0.538  5.935   29.3  4.4986    4  307.0   \n",
            "..        ...  ...    ...   ...    ...    ...    ...     ...  ...    ...   \n",
            "434  13.91340  0.0  18.10     0  0.713  6.208   95.0  2.2222   24  666.0   \n",
            "115   0.17134  0.0  10.01     0  0.547  5.928   88.2  2.4631    6  432.0   \n",
            "156   2.44668  0.0  19.58     0  0.871  5.272   94.0  1.7364    5  403.0   \n",
            "402   9.59571  0.0  18.10     0  0.693  6.404  100.0  1.6390   24  666.0   \n",
            "494   0.27957  0.0   9.69     0  0.585  5.926   42.6  2.3817    6  391.0   \n",
            "\n",
            "     PTRATIO       B  LSTAT  \n",
            "22      21.0  396.90  18.72  \n",
            "322     19.6  396.90   7.70  \n",
            "417     20.2  127.36  26.64  \n",
            "48      17.9  396.90  30.81  \n",
            "16      21.0  386.85   6.58  \n",
            "..       ...     ...    ...  \n",
            "434     20.2  100.63  15.17  \n",
            "115     17.8  344.91  15.76  \n",
            "156     14.7   88.63  16.14  \n",
            "402     20.2  376.11  20.31  \n",
            "494     19.2  396.90  13.59  \n",
            "\n",
            "[379 rows x 13 columns]\n",
            "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
            "22  -0.280488 -0.458066 -0.478013 -0.265396 -0.157784 -0.169144  0.808787   \n",
            "322 -0.381392 -0.458066 -0.590046 -0.265396 -0.548531 -0.317089 -0.679898   \n",
            "417  2.548347 -0.458066  0.990203 -0.265396  1.066559 -1.396650  0.716189   \n",
            "48  -0.392528 -0.458066 -0.659329 -0.265396 -0.939279 -1.257493  0.936999   \n",
            "16  -0.300929 -0.458066 -0.478013 -0.265396 -0.157784 -0.472359 -1.413557   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "434  1.171352 -0.458066  0.990203 -0.265396  1.361791 -0.072467  0.926315   \n",
            "115 -0.401977 -0.458066 -0.202354 -0.265396 -0.079634 -0.482612  0.684136   \n",
            "156 -0.141473 -0.458066  1.208372 -0.265396  2.733749 -1.443524  0.890700   \n",
            "402  0.677020 -0.458066  0.990203 -0.265396  1.188125  0.214634  1.104387   \n",
            "494 -0.389586 -0.458066 -0.249526 -0.265396  0.250331 -0.485542 -0.939884   \n",
            "\n",
            "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
            "22   0.130618 -0.631238 -0.601135  1.156079  0.438279  0.837817  \n",
            "322  0.504303 -0.516714 -0.719942  0.478795  0.438279 -0.713354  \n",
            "417 -1.039042  1.659227  1.531451  0.769060 -2.564252  1.952634  \n",
            "48   1.081199 -0.745761 -1.040722 -0.343620  0.438279  2.539602  \n",
            "16   0.392579 -0.631238 -0.601135  1.156079  0.326328 -0.871005  \n",
            "..        ...       ...       ...       ...       ...       ...  \n",
            "434 -0.750468  1.659227  1.531451  0.769060 -2.862009  0.338120  \n",
            "115 -0.629505 -0.402191  0.141409 -0.391997 -0.140861  0.421169  \n",
            "156 -0.994403 -0.516714 -0.030862 -1.891695 -2.995683  0.474657  \n",
            "402 -1.043310  1.659227  1.531451  0.769060  0.206690  1.061625  \n",
            "494 -0.670379 -0.402191 -0.102146  0.285286  0.438279  0.115720  \n",
            "\n",
            "[379 rows x 13 columns]\n",
            "[ 2.22216359e+01 -1.04584156e+00  9.60246703e-01 -5.73183964e-03\n",
            "  9.49897190e-01 -2.04817912e+00  2.38780141e+00 -1.35594192e-01\n",
            " -3.08646486e+00  2.87698713e+00 -2.28153063e+00 -2.12864445e+00\n",
            "  8.16299519e-01 -3.46938630e+00]\n",
            "[672053.9704720606]\n",
            "[662089.5398626865]\n",
            "[ 4.06344437e+01 -1.19774285e-01  4.27606829e-02 -6.60846133e-04\n",
            "  3.82107939e+00 -1.77867027e+01  3.49714137e+00 -4.81969263e-03\n",
            " -1.54977882e+00  3.29938217e-01 -1.35788510e-02 -1.02985492e+00\n",
            "  9.09358470e-03 -4.88378044e-01]\n",
            "----------------------------\n",
            "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
            "18   0.80271   0.0   8.14     0  0.538  5.456  36.6  3.7965    4  307.0   \n",
            "70   0.08826   0.0  10.81     0  0.413  6.417   6.6  5.2873    4  305.0   \n",
            "370  6.53876   0.0  18.10     1  0.631  7.016  97.5  1.2024   24  666.0   \n",
            "295  0.12932   0.0  13.92     0  0.437  6.678  31.1  5.9604    4  289.0   \n",
            "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
            "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
            "305  0.05479  33.0   2.18     0  0.472  6.616  58.1  3.3700    7  222.0   \n",
            "114  0.14231   0.0  10.01     0  0.547  6.254  84.2  2.2565    6  432.0   \n",
            "22   1.23247   0.0   8.14     0  0.538  6.142  91.7  3.9769    4  307.0   \n",
            "274  0.05644  40.0   6.41     1  0.447  6.758  32.9  4.0776    4  254.0   \n",
            "154  1.41385   0.0  19.58     1  0.871  6.129  96.0  1.7494    5  403.0   \n",
            "\n",
            "     PTRATIO       B  LSTAT  \n",
            "18      21.0  288.99  11.69  \n",
            "70      19.2  383.73   6.72  \n",
            "370     20.2  392.05   2.96  \n",
            "295     16.0  396.90   6.27  \n",
            "2       17.8  392.83   4.03  \n",
            "..       ...     ...    ...  \n",
            "305     18.4  393.36   8.93  \n",
            "114     17.8  388.74  10.45  \n",
            "22      21.0  396.90  18.72  \n",
            "274     17.6  396.90   3.53  \n",
            "154     14.7  321.02  15.12  \n",
            "\n",
            "[379 rows x 13 columns]\n",
            "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
            "18  -0.316447 -0.470215 -0.426697 -0.271035 -0.132445 -1.224429 -1.112008   \n",
            "70  -0.395698 -0.470215 -0.040597 -0.271035 -1.222028  0.197384 -2.167711   \n",
            "370  0.319831 -0.470215  1.013587  3.679823  0.678205  1.083614  1.031070   \n",
            "295 -0.391143 -0.470215  0.409131 -0.271035 -1.012828  0.583538 -1.305553   \n",
            "2   -0.402461 -0.470215 -0.581426 -0.271035 -0.733895  1.333652 -0.249850   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "305 -0.399411  1.000948 -1.288554 -0.271035 -0.707745  0.491808 -0.355420   \n",
            "114 -0.389702 -0.470215 -0.156282 -0.271035 -0.053995 -0.043776  0.563042   \n",
            "22  -0.268775 -0.470215 -0.426697 -0.271035 -0.132445 -0.209482  0.826968   \n",
            "274 -0.399228  1.313013 -0.676867  3.679823 -0.925661  0.701899 -1.242211   \n",
            "154 -0.248655 -0.470215  1.227605  3.679823  2.770203 -0.228716  0.978285   \n",
            "\n",
            "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
            "18   0.000251 -0.615379 -0.577477  1.174560 -0.789528 -0.124028  \n",
            "70   0.697054 -0.615379 -0.589328  0.322492  0.289460 -0.815957  \n",
            "370 -1.212237  1.702998  1.549887  0.795863  0.384216 -1.339429  \n",
            "295  1.011662 -0.615379 -0.684141 -1.192295  0.439452 -0.878607  \n",
            "2    0.547391 -0.847217 -0.962654 -0.340228  0.393099 -1.190462  \n",
            "..        ...       ...       ...       ...       ...       ...  \n",
            "305 -0.199096 -0.267623 -1.081170 -0.056205  0.399135 -0.508278  \n",
            "114 -0.719549 -0.383542  0.163249 -0.340228  0.346518 -0.296662  \n",
            "22   0.084570 -0.615379 -0.577477  1.174560  0.439452  0.854697  \n",
            "274  0.131637 -0.615379 -0.891544 -0.434902  0.439452 -1.260073  \n",
            "154 -0.956568 -0.499461 -0.008599 -1.807678 -0.424740  0.353500  \n",
            "\n",
            "[379 rows x 13 columns]\n",
            "[22.60263852 -1.09354629  0.77488768  0.1199992   0.87491798 -1.87316664\n",
            "  3.19757107  0.09938661 -2.83619912  2.40859792 -1.66039493 -1.91832572\n",
            "  0.70131592 -3.74017156]\n",
            "[335369.26589069923]\n",
            "[309784.2773795316]\n",
            "[ 2.82415240e+01 -1.21317507e-01  3.45546688e-02  1.74620839e-02\n",
            "  3.45623534e+00 -1.63294437e+01  4.73061245e+00  3.50421167e-03\n",
            " -1.32559928e+00  2.79403270e-01 -9.85076093e-03 -9.08109832e-01\n",
            "  7.98735129e-03 -5.20730219e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm = Normalizer()\n",
        "#norm.predict(X_test)"
      ],
      "metadata": {
        "id": "JBYlHTRjodA9"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HVfnXvZFi98"
      },
      "source": [
        "## Exercise 2 - Ridge Linear Regression\n",
        "\n",
        "Recall that ridge regression is identical to OLS but with a L2 penalty over the weights:\n",
        "\n",
        "$L(y,\\hat{y})=\\sum_{i=1}^{i=N}{(y^{(i)}-\\hat{y}^{(i)})^2} + \\lambda \\left\\Vert w \\right\\Vert_2^2$\n",
        "\n",
        "where $y^{(i)}$ is the **true** value and $\\hat{y}^{(i)}$ is the **predicted** value of the $i_{th}$ example, and $N$ is the number of examples\n",
        "\n",
        "* Show, by differentiating the above loss, that the analytical solution is $w_{Ridge}=(X^TX+\\lambda I)^{-1}X^Ty$\n",
        "* Change `OrdinaryLinearRegression` and `OrdinaryLinearRegressionGradientDescent` classes to work also for ridge regression. Either add a parameter, or use inheritance.\n",
        "* **Bonus: Noise as a regularizer**: Show that OLS (ordinary least square), if one adds multiplicative noise to the features the **average** solution for $W$ is equivalent to Ridge regression. In other words, if $X'= X*G$ where $G$ is an uncorrelated noise with variance $\\sigma$ and mean 1, then solving for $X'$ with OLS is like solving Ridge for $X$. What is the interpretation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L = (Xw - y)^T(Xw - y) + λw^Tw → min$\n",
        "\n",
        "$∇_w L = X^T(Xw-y) + (Xw-y)^TX + λ(I^Tw + w^TI) = X^TXw-X^Ty + X^T(Xw-y) +λ(I^Tw+I^Tw) =$\n",
        "$= 2X^TXw-2X^Ty+2λI^Tw = 0$\n",
        "\n",
        "$(X^TX+λI^T)w = X^Ty$\n",
        "\n",
        "$w = (X^TX + λI^T)^{-1}X^Ty$\n",
        "\n",
        "$w = (X^TX + λI)^{-1}X^Ty$"
      ],
      "metadata": {
        "id": "joivmdaAFqc6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4v0foNX1eiu"
      },
      "outputs": [],
      "source": [
        "class RidgeLs(Ols):\n",
        "  def __init__(self, ridge_lambda, *wargs, **kwargs):\n",
        "    super(RidgeLs,self).__init__(*wargs, **kwargs)\n",
        "    self.ridge_lambda = ridge_lambda\n",
        "\n",
        "  def _fit(self, X, Y):\n",
        "    #Closed form of ridge regression\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAj1Qxoc1eiu"
      },
      "source": [
        "### Use scikitlearn implementation for OLS, Ridge and Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GozQEKRw1eiu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}