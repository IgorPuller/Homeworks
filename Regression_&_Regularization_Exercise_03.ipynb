{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chEqqJbLzFew"
      },
      "source": [
        "# Ydata Data Science School\n",
        "## Linear Regression & Regularization Exercise.\n",
        "\n",
        "\n",
        "## Outline\n",
        "In this exercise you will learn the following topics:\n",
        "\n",
        "1. Refresher on how linear regression is solved in batch and in Gradient Descent\n",
        "2. Implementation of Ridge Regression\n",
        "3. Comparing Ridge, Lasso and vanila Linear Regression on a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9UFmk2greT"
      },
      "source": [
        "## Refresher on Ordinary Least Square (OLS) aka Linear Regeression\n",
        "\n",
        "### Lecture Note\n",
        "\n",
        "In Matrix notation, the matrix $X$ is of dimensions $n \\times p$ where each row is an example and each column is a feature dimension.\n",
        "\n",
        "Similarily, $y$ is of dimension $n \\times 1$ and $w$ is of dimensions $p \\times 1$.\n",
        "\n",
        "The model is $\\hat{y}=X\\cdot w$ where we assume for simplicity that $X$'s first columns equals to 1 (one padding), to account for the bias term.\n",
        "\n",
        "Our objective is to optimize the loss $L$ defines as resiudal sum of squares (RSS):\n",
        "\n",
        "$L_{RSS}=\\frac{1}{N}\\left\\Vert Xw-y \\right\\Vert^2$ (notice that in matrix notation this means summing over all examples, so $L$ is scalar.)\n",
        "\n",
        "To find the optimal $w$ one needs to derive the loss with respect to $w$.\n",
        "\n",
        "$\\frac{\\partial{L_{RSS}}}{\\partial{w}}=\\frac{2}{N}X^T(Xw-y)$ (to see why, read about [matrix derivatives](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf) or see class notes )\n",
        "\n",
        "Thus, the gardient descent solution is $w'=w-\\alpha \\frac{2}{N}X^T(Xw-y)$.\n",
        "\n",
        "Solving $\\frac{\\partial{L_{RSS}}}{\\partial{w}}=0$ for $w$ one can also get analytical solution:\n",
        "\n",
        "$w_{OLS}=(X^TX)^{-1}X^Ty$\n",
        "\n",
        "The first term, $(X^TX)^{-1}X^T$ is also called the pseudo inverse of $X$.\n",
        "\n",
        "See [lecture note from Stanford](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA3MEKz80vdy"
      },
      "source": [
        "## Exercise 1 - Ordinary Least Square\n",
        "* Get the boston housing dataset https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n",
        "\n",
        "* What is $p$? what is $n$ in the above notation? hint: [shape](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ndarray.shape.html)\n",
        "\n",
        "* write a model `OrdinaryLinearRegression` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score` (which returns the MSE on a given sample set). Hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.\n",
        "\n",
        "* Fit the model. What is the training MSE?\n",
        "\n",
        "* Plot a scatter plot where on x-axis plot $Y$ and in the y-axis $\\hat{Y}_{OLS}$\n",
        "\n",
        "* Split the data to 75% train and 25% test 20 times. What is the average MSE now for train and test? Hint: use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) or [ShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html).\n",
        "\n",
        "* Use a t-test to proove that the MSE for training is significantly smaller than for testing. What is the p-value? Hint: use [scipy.stats.ttest_rel](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html).\n",
        "\n",
        "* Write a new class `OrdinaryLinearRegressionGradientDescent` which inherits from `OrdinaryLinearRegression` and solves the problem using gradinet descent. The class should get as a parameter the learning rate and number of iteration. Plot the class convergance. What is the effect of learning rate? How would you find number of iteration automatically? Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your features first.\n",
        "\n",
        "* The following parameters are optional (not mandatory to use):\n",
        "    * early_stop - True / False boolean to indicate to stop running when loss stops decaying and False to continue.\n",
        "    * verbose- True/False boolean to turn on / off logging, e.g. print details like iteration number and loss (https://en.wikipedia.org/wiki/Verbose_mode)\n",
        "    * track_loss - True / False boolean when to save loss results to present later in learning curve graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZuSS8LhcfZdn"
      },
      "outputs": [],
      "source": [
        "# * write a model `Ols` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score`.? hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.\n",
        "import numpy as np\n",
        "class Ols(object):\n",
        "  def __init__(self):\n",
        "    self.w = None\n",
        "\n",
        "  @staticmethod\n",
        "  def pad(X):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    #remeber pad with 1 before fitting\n",
        "    if X.ndim == 1:\n",
        "      X = np.expand_dims(X,axis=-1)\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    self.w = np.linalg.pinv(X.T@X)@X.T@Y\n",
        "\n",
        "\n",
        "  def _fit(self, X, Y):\n",
        "    # optional to use this\n",
        "    pass\n",
        "\n",
        "  def predict(self, X):\n",
        "    #return wx\n",
        "    if X.ndim == 1:\n",
        "      X = np.expand_dims(X,axis=-1)\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    w = self.w\n",
        "    w = np.expand_dims(w,axis=-1)\n",
        "    return X@w\n",
        "\n",
        "  def _predict(self, X):\n",
        "    # optional to use this\n",
        "    pass\n",
        "\n",
        "  def score(self, X, Y):\n",
        "    #return MSE\n",
        "    if X.ndim == 1:\n",
        "      X = np.expand_dims(X,axis=-1)\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    n = Y.shape[0]\n",
        "    w = self.w\n",
        "    #w = np.expand_dims(w,axis=-1)\n",
        "    return np.sum((X @ w - Y)**2)/n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "np.random.seed(42)\n",
        "data = pd.read_csv('/content/Boston-house-price-data.csv')\n",
        "data.head()\n",
        "X = data.iloc[:,:-1]\n",
        "Y = data.iloc[:,-1]\n",
        "#X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)"
      ],
      "metadata": {
        "id": "hx3ar6H7N0uW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "model = Ols();\n",
        "model.fit(X_train,Y_train);\n",
        "model.predict(X_test);\n",
        "print(model.score(X_train,Y_train))\n",
        "model.score(X_test,Y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvPmt-zjPSgq",
        "outputId": "e05e22b8-03b3-439d-ba20-d9ba0781b99f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.545481487421423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.51744423120909"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(Y_test, model.predict(X_test));\n",
        "plt.xlabel('$Y$')\n",
        "plt.ylabel('$\\hat{Y}_{OLS}$');\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "O4eS4JUVdeEn",
        "outputId": "8fd2236f-1087-4192-c45a-34c14d0b4eb6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_train = []\n",
        "mse_test = []\n",
        "for i in range(20):\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
        "  model = Ols();\n",
        "  model.fit(X_train,Y_train);\n",
        "  model.predict(X_test);\n",
        "  #w=model.w\n",
        "  #w = np.expand_dims(w,axis=-1)\n",
        "  #print(np.sum((X_test@w-Y_test)**2)/Y_test.size)\n",
        "  #print(np.sum((X_train@w-Y_test)**2)/Y_train.size)\n",
        "  mse_train.append(model.score(X_train,Y_train))\n",
        "  mse_test.append(model.score(X_test,Y_test))\n",
        "  #print(model.w)\n",
        "\n",
        "#w=model.w\n",
        "#w = np.expand_dims(w,axis=-1)\n",
        "#print(w)\n",
        "#print(np.sum((X_test@w-Y_test)**2)/Y_test.size)\n",
        "#print(np.sum((X_train@w-Y_test)**2)/Y_train.size)\n",
        "print('MSE on test:',sum(mse_test)/len(mse_test))\n",
        "print('MSE on train:',sum(mse_train)/len(mse_train))\n",
        "#Y_train, model.predict(X_train)\n",
        "#Y_test, model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP2LKhKkeXPR",
        "outputId": "4fd9845a-329c-4274-967c-381ce237b700"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE on test: 25.507532313877245\n",
            "MSE on train: 21.19393351376658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "stats.ttest_rel(mse_train, mse_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyGF5ldIgWzL",
        "outputId": "90845c59-69aa-45ea-8418-f099b43a8078"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TtestResult(statistic=-2.3227663224081145, pvalue=0.03144792965171587, df=19)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B689kXfa1eit"
      },
      "outputs": [],
      "source": [
        "# Write a new class OlsGd which solves the problem using gradinet descent.\n",
        "# The class should get as a parameter the learning rate and number of iteration.\n",
        "# Plot the loss convergance. for each alpha, learning rate plot the MSE with respect to number of iterations.\n",
        "# What is the effect of learning rate?\n",
        "# How would you find number of iteration automatically?\n",
        "# Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your feature first.\n",
        "class Normalizer():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X):\n",
        "    #self.X_min = X.T.min(axis=1)\n",
        "    #self.X_max = X.T.max(axis=1)\n",
        "    self.X_mean = X.T.mean(axis=1)\n",
        "    self.X_std = X.T.std(axis=1)\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    #apply normalization\n",
        "    #return (X-self.X_min)/(self.X_max - self.X_min)\n",
        "    return (X-self.X_mean)/self.X_std\n",
        "\n",
        "class OlsGd(Ols):\n",
        "\n",
        "  def __init__(self, learning_rate=.05,\n",
        "               num_iteration=1000,\n",
        "               normalize=True,\n",
        "               early_stop=True,\n",
        "               verbose=True):\n",
        "\n",
        "    super(OlsGd, self).__init__()\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iteration = num_iteration\n",
        "    self.early_stop = early_stop\n",
        "    self.normalize = normalize\n",
        "    self.normalizer = Normalizer()\n",
        "    self.verbose = verbose\n",
        "\n",
        "\n",
        "  def _fit(self, X, Y, reset=True, track_loss=True):\n",
        "    #remeber to normalize the data before starting\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    norm_X = Normalizer()\n",
        "    norm_X.fit(X)\n",
        "    X = norm_X.predict(X)\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    n = Y.shape[0]\n",
        "    w = np.random.uniform(-1, 1, (X.shape[1]))\n",
        "    for i in range(self.num_iteration):\n",
        "      grad = 2/n * X.T @ (X @ w - Y)\n",
        "      w = w - self.learning_rate*grad\n",
        "    self.w = w\n",
        "\n",
        "\n",
        "  def _predict(self, X):\n",
        "    #remeber to normalize the data before starting\n",
        "    norm = Normalizer()\n",
        "    norm.fit(self.X)\n",
        "    X = norm.predict(X)\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    w = self.w\n",
        "    w = np.expand_dims(w,axis=-1)\n",
        "    return X@w\n",
        "\n",
        "  def _step(self, X, Y):\n",
        "    # use w update for gradient descent\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(\"----------------------------\")\n",
        "  mse_train = []\n",
        "  mse_test = []\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
        "  model = OlsGd(learning_rate=.05, num_iteration=1000);\n",
        "  model._fit(X_train,Y_train);\n",
        "  model._predict(X_test);\n",
        "  norm = Normalizer()\n",
        "  norm.fit(X_train)\n",
        "  X_train = norm.predict(X_train)\n",
        "  X_test = norm.predict(X_test)\n",
        "  mse_train.append(model.score(X_train,Y_train))\n",
        "  mse_test.append(model.score(X_test,Y_test))\n",
        "  print('коэфициэнты Градиентного спуска')\n",
        "  print(model.w)\n",
        "  #print(mse_test)\n",
        "  #print(mse_train)\n",
        "  norm = Normalizer()\n",
        "  norm.fit(X_train)\n",
        "  X_train = norm.predict(X_train)\n",
        "  model = Ols();\n",
        "  model.fit(X_train,Y_train);\n",
        "  model.predict(X_test);\n",
        "  print('коэфициэнты аналитического решения')\n",
        "  print(model.w)\n",
        "  mse_O = model.score(X_train,Y_train)\n",
        "\n",
        "\n",
        "mse_test,mse_train,mse_O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkHwrVH_8Up2",
        "outputId": "f9d7efd7-1113-409f-bcc7-bfccd3f91ec0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "коэфициэнты Градиентного спуска\n",
            "[22.60263852 -1.09354629  0.77488768  0.1199992   0.87491798 -1.87316664\n",
            "  3.19757107  0.09938661 -2.83619912  2.40859792 -1.66039493 -1.91832572\n",
            "  0.70131592 -3.74017156]\n",
            "коэфициэнты аналитического решения\n",
            "[22.60263852 -1.09367799  0.775104    0.12075558  0.87480615 -1.87336001\n",
            "  3.19740875  0.09957944 -2.83610046  2.41033414 -1.66234982 -1.91838937\n",
            "  0.70132568 -3.7403098 ]\n",
            "----------------------------\n",
            "коэфициэнты Градиентного спуска\n",
            "[22.53720317 -1.10178459  1.09515001 -0.12525499  0.63501703 -1.69059842\n",
            "  2.82288538 -0.20708979 -3.12218402  2.78742627 -2.30805874 -2.01892319\n",
            "  0.80277692 -3.51269932]\n",
            "коэфициэнты аналитического решения\n",
            "[22.53720317 -1.10202904  1.09560986 -0.12365487  0.63470585 -1.69065741\n",
            "  2.82243745 -0.20675959 -3.12184189  2.79237471 -2.31351557 -2.01918977\n",
            "  0.8027837  -3.51316798]\n",
            "----------------------------\n",
            "коэфициэнты Градиентного спуска\n",
            "[ 2.24234828e+01 -1.06046561e+00  9.80472836e-01 -2.46362955e-02\n",
            "  7.51077063e-01 -2.17755746e+00  2.64477876e+00  2.23734095e-02\n",
            " -3.02417041e+00  2.80253351e+00 -2.16132764e+00 -2.05816505e+00\n",
            "  6.92200160e-01 -3.67358919e+00]\n",
            "коэфициэнты аналитического решения\n",
            "[ 2.24234828e+01 -1.06089490e+00  9.81344518e-01 -2.20072854e-02\n",
            "  7.50690966e-01 -2.17790615e+00  2.64435907e+00  2.25039731e-02\n",
            " -3.02426041e+00  2.80935555e+00 -2.16924231e+00 -2.05827415e+00\n",
            "  6.92278047e-01 -3.67378192e+00]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([23.581333973867768], [21.417487697087516], 21.417480948900288)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm = Normalizer()\n",
        "#norm.predict(X_test)"
      ],
      "metadata": {
        "id": "JBYlHTRjodA9"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HVfnXvZFi98"
      },
      "source": [
        "## Exercise 2 - Ridge Linear Regression\n",
        "\n",
        "Recall that ridge regression is identical to OLS but with a L2 penalty over the weights:\n",
        "\n",
        "$L(y,\\hat{y})=\\sum_{i=1}^{i=N}{(y^{(i)}-\\hat{y}^{(i)})^2} + \\lambda \\left\\Vert w \\right\\Vert_2^2$\n",
        "\n",
        "where $y^{(i)}$ is the **true** value and $\\hat{y}^{(i)}$ is the **predicted** value of the $i_{th}$ example, and $N$ is the number of examples\n",
        "\n",
        "* Show, by differentiating the above loss, that the analytical solution is $w_{Ridge}=(X^TX+\\lambda I)^{-1}X^Ty$\n",
        "* Change `OrdinaryLinearRegression` and `OrdinaryLinearRegressionGradientDescent` classes to work also for ridge regression. Either add a parameter, or use inheritance.\n",
        "* **Bonus: Noise as a regularizer**: Show that OLS (ordinary least square), if one adds multiplicative noise to the features the **average** solution for $W$ is equivalent to Ridge regression. In other words, if $X'= X*G$ where $G$ is an uncorrelated noise with variance $\\sigma$ and mean 1, then solving for $X'$ with OLS is like solving Ridge for $X$. What is the interpretation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L = (Xw - y)^T(Xw - y) + λw^Tw → min$\n",
        "\n",
        "$∇_w L = X^T(Xw-y) + (Xw-y)^TX + λ(I^Tw + w^TI) = X^TXw-X^Ty + X^T(Xw-y) +λ(I^Tw+I^Tw) =$\n",
        "$= 2X^TXw-2X^Ty+2λI^Tw = 0$\n",
        "\n",
        "$(X^TX+λI^T)w = X^Ty$\n",
        "\n",
        "$w = (X^TX + λI^T)^{-1}X^Ty$\n",
        "\n",
        "$w = (X^TX + λI)^{-1}X^Ty$"
      ],
      "metadata": {
        "id": "joivmdaAFqc6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4v0foNX1eiu"
      },
      "outputs": [],
      "source": [
        "class RidgeLs(Ols):\n",
        "  def __init__(self, ridge_lambda, *wargs, **kwargs):\n",
        "    super(RidgeLs,self).__init__(*wargs, **kwargs)\n",
        "    self.ridge_lambda = ridge_lambda\n",
        "\n",
        "  def _fit(self, X, Y):\n",
        "    #Closed form of ridge regression\n",
        "    X = np.pad(array=X, pad_width=((0,0),(1,0)), mode='constant',constant_values=1)\n",
        "    self.w = np.linalg.pinv(X.T@X+self.ridge_lambda*np.identity(X.shape[1]))@X.T@Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAj1Qxoc1eiu"
      },
      "source": [
        "### Use scikitlearn implementation for OLS, Ridge and Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GozQEKRw1eiu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}